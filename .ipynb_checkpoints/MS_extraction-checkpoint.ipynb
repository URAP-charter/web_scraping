{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import os, csv\n",
    "# other options:   import os.system(wget http)   OR   import urllib.urlretrieve (this is an alternative command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_20\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_20\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 OLD CLEMSON RD, COLUMBIA, SC \n",
      " https://www.richland2.org/charterhigh/ \n",
      " {'AMUGM': '-2', 'TR02F': '-2', 'HISP': '5', 'WHUGF': '-2', 'BLUGM': '-2', 'TRPKM': '-2', 'BL10M': '1', 'STID': '4002', 'AM06M': '-2', 'KG': '-2', 'AS10M': '0', 'HI02F': '-2', 'WHKGM': '-2', 'AS02M': '-2', 'HIKGM': '-2', 'HP05M': '-2', 'TR07F': '-2', 'BL11F': '9', 'HPKGF': '-2', 'WHALF': '13', 'BL09M': '1', 'BLALM': '16', 'TR12F': '0', 'WH12M': '4', 'G08': '-2', 'HIPKF': '-2', 'WH11M': '3', 'BLPKF': '-2', 'LATCOD': '34.1231', 'WH10F': '0', 'HI12M': '0', 'UG': '-2', 'SHARED': '2', 'AM01F': '-2', 'AS05F': '-2', 'G01': '-2', 'BL02F': '-2', 'HP07M': '-2', 'ASPKM': '-2', 'AM09F': '0', 'LSTREE': '750 OLD CLEMSON RD', 'HPPKF': '-2', 'BL02M': '-2', 'G11OFFRD': '1', 'ISMEMPUP': 'PS', 'AM12F': '0', 'FRELCH': '12', 'TRUGM': '-2', 'HIUGM': '-2', 'WH01F': '-2', 'G07OFFRD': '2', 'NSLPSTATUS': 'M', 'BL05F': '-2', 'HP09M': '0', 'AM05M': '-2', 'MCITY': 'COLUMBIA', 'SCHNO': '1554', 'MZIP': '29229', 'BL09F': '1', 'WH05F': '-2', 'HI04M': '-2', 'NCESSCH': '4.50E+11', 'BLKGM': '-2', 'G06OFFRD': '2', 'FTE': '1', 'TR10M': '0', 'BL10F': '3', 'HI07M': '-2', 'AS08M': '-2', 'AMPKM': '-2', 'BL07F': '-2', 'AS03M': '-2', 'AS09M': '0', 'TR08M': '-2', 'BL01F': '-2', 'HP02M': '-2', 'AM02M': '-2', 'AM04F': '-2', 'LZIP4': '0', 'WHKGF': '-2', 'BLKGF': '-2', 'AM08F': '-2', 'BL04M': '-2', 'BLACK': '39', 'TR01F': '-2', 'SEARCH PARAMS': 'RICHLAND TWO CHARTER HIGH 750 OLD CLEMSON RD, COLUMBIA, SC', 'GSLO': '9', 'AMKGM': '-2', 'TR': '3', 'PHONE': '8034191348', 'G12': '31', 'WH04M': '-2', 'TR12M': '1', 'AM02F': '-2', 'SMEMPUP': '2', 'HP01F': '-2', 'HP10M': '0', 'SEASCH': '600', 'TR11F': '2', 'HI01F': '-2', 'CONAME': 'RICHLAND COUNTY', 'CHARTAUTH2': 'M', 'TRKGF': '-2', 'WHUGM': '-2', 'ASKGM': '-2', 'HI10M': '1', 'HIALM': '1', 'AS04F': '-2', 'RDM #': '0.934345634', 'AMKGF': '-2', 'WGET INDEX': '1', 'HIALF': '4', 'MAGNET': '2', 'AM07M': '-2', 'G07': '-2', 'ASPKF': '-2', 'TRALM': '1', 'WH06M': '-2', 'HP03F': '-2', 'WH08M': '-2', 'LEANM': 'RICHLAND 02', 'HP12F': '0', 'G02OFFRD': '2', 'AM04M': '-2', 'HIPKM': '-2', 'WH01M': '-2', 'HI05M': '-2', 'AS04M': '-2', 'AM': '0', 'AM09M': '0', 'HP08M': '-2', 'HP05F': '-2', 'TRKGM': '-2', 'RECONSTF': '2', 'TOTFRL': '16', 'ASALF': '1', 'HP11F': '0', 'RECONSTY': 'N', 'LONCOD': '-80.8626', 'AMPKF': '-2', 'WHPKF': '-2', 'SPFEMALE': '2', 'BL04F': '-2', 'HI07F': '-2', 'G02': '-2', 'SPELM': '2', 'ISFTEPUP': 'PS', 'AM12M': '0', 'TR07M': '-2', 'HI08F': '-2', 'SFLE': '2', 'TR03F': '-2', 'HP04F': '-2', 'BL01M': '-2', 'WH03F': '-2', 'AS06F': '-2', 'G05': '-2', 'SURVYEAR': '2013', 'LCITY': 'COLUMBIA', 'HI06F': '-2', 'HPUGM': '-2', 'MZIP4': '0', 'ASUGM': '-2', 'TITLEISTAT': '6', 'WH02F': '-2', 'G08OFFRD': '2', 'HI01M': '-2', 'G04': '-2', 'STITLI': 'N', 'LEVEL': '3', 'WH03M': '-2', 'HP10F': '0', 'G11': '28', 'TR05F': '-2', 'AS03F': '-2', 'AS01F': '-2', 'ISPFEMALE': 'PS', 'HP09F': '0', 'HI10F': '0', 'HPALM': '0', 'TR04F': '-2', 'AM08M': '-2', 'BLALF': '23', 'WH11F': '6', 'HI02M': '-2', 'BL11M': '7', 'HI09M': '0', 'URL': 'https://www.richland2.org/charterhigh/', 'HI03F': '-2', 'WH07M': '-2', 'ISPELM': 'PS', 'ISFLE': 'PS', 'SCHNAM': 'RICHLAND TWO CHARTER HIGH', 'BLPKM': '-2', 'HI06M': '-2', 'BL08M': '-2', 'AS12M': '0', 'REDLCH': '4', 'ASALM': '0', 'LSTATE': 'SC', 'G09': '3', 'G12OFFRD': '1', 'TR01M': '-2', 'AS07M': '-2', 'TRPKF': '-2', 'TR10F': '0', 'AM07F': '-2', 'HPKGM': '-2', 'TR09F': '0', 'WH04F': '-2', 'UGOFFRD': '2', 'HI08M': '-2', 'AS11M': '0', 'SCHOOL ID': 'SC600', 'G04OFFRD': '2', 'AS05M': '-2', 'STATUS': '1', 'VIRTUALSTAT': 'VIRTUALNO', 'HP06F': '-2', 'WH10M': '1', 'WH09M': '0', 'WHPKM': '-2', 'PACIFIC': '0', 'ADDRESS': '750 OLD CLEMSON RD, COLUMBIA, SC', 'TR05M': '-2', 'UNION': '0', 'AM10F': '0', 'SFTEPUP': '2', 'WH07F': '-2', 'TR11M': '0', 'AS11F': '0', 'BIES': '2', 'TITLEI': '2', 'HI04F': '-2', 'BL03M': '-2', 'ASUGF': '-2', 'SPWHITE': '2', 'CONUM': '45079', 'AS02F': '-2', 'AMALF': '0', 'TR06M': '-2', 'HI11M': '0', 'HI09F': '0', 'AM01M': '-2', 'CHARTAUTH1': '4002', 'TR04M': '-2', 'AS08F': '-2', 'WHALM': '8', 'HPALF': '0', 'CHARTR': '1', 'BL12F': '10', 'WH08F': '-2', 'KGOFFRD': '2', 'BL08F': '-2', 'HPUGF': '-2', 'AMUGF': '-2', 'TOTETH': '69', 'AS09F': '0', 'AM03M': '-2', 'PKOFFRD': '2', 'AS06M': '-2', 'HP01M': '-2', 'HP03M': '-2', 'G03': '-2', 'TRUGF': '-2', 'AM03F': '-2', 'AS07F': '-2', 'HI03M': '-2', 'AMALM': '0', 'TR09M': '0', 'TRALF': '2', 'HP12M': '0', 'HI11F': '1', 'HIUGF': '-2', 'WH02M': '-2', 'ASKGF': '-2', 'BLUGF': '-2', 'G10': '7', 'WH12F': '6', 'PK': '-2', 'WHITE': '21', 'HP06M': '-2', 'TR02M': '-2', 'AS12F': '0', 'MSTATE': 'SC', 'WH05M': '-2', 'TYPE': '1', 'MSTREE': '750 OLD CLEMSON ROAD', 'HP07F': '-2', 'G10OFFRD': '1', 'BL03F': '-2', 'CDCODE': '4502', 'ISPWHITE': 'PS', 'LEAID': '4503390', 'G01OFFRD': '2', 'AM05F': '-2', 'G06': '-2', 'G05OFFRD': '2', 'HI12F': '3', 'AM11F': '0', 'TR08F': '-2', 'BL06M': '-2', 'HP08F': '-2', 'HP11M': '0', 'TR06F': '-2', 'WH06F': '-2', 'AM06F': '-2', 'LZIP': '29229', 'AS10F': '1', 'HPPKM': '-2', 'BL12M': '7', 'WAYBACK URL': 'https://web.archive.org/web/*/https://www.richland2.org/charterhigh', 'FIPST': '45', 'HIKGF': '-2', 'BL07M': '-2', 'AS01M': '-2', 'ULOCAL': '21', 'G03OFFRD': '2', 'HP04M': '-2', 'BL05M': '-2', 'BL06F': '-2', 'ASIAN': '1', 'HI05F': '-2', 'AM11M': '0', 'TR03M': '-2', 'WH09F': '1', 'HP02F': '-2', 'G09OFFRD': '1', 'MEMBER': '69', 'AM10M': '0', 'GSHI': '12'}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the contents of our list called sample--just the first entry\n",
    "print(sample[0][\"ADDRESS\"], \"\\n\", sample[0][\"URL\"], \"\\n\", sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.richland2.org/charterhigh/', 'https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'https://www.nhaschools.com/schools/rivercity/Pages/default.aspx'] \n",
      " ['RICHLAND TWO CHARTER HIGH', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL', 'RIVER CITY SCHOLARS CHARTER ACADEMY'] \n",
      " ['750 OLD CLEMSON RD, COLUMBIA, SC', '3425 WINTER LK RD LAC1200, WINTER HAVEN, FL', '944 EVERGREEN ST, GRAND RAPIDS, MI']\n"
     ]
    }
   ],
   "source": [
    "# it's VERY important that these three lists be indexed the same, so let's check:\n",
    "print(url_list[:3], \"\\n\", name_list[:3], \"\\n\", terms_list[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Looking at the above, the 3 lists with URLs, names, and search terms, respectively, seem to line up as we would expect: URL[0] corresponds to name[0] corresponds to terms[0], and so on. Now, let's turn these lists into tuples so we can use them for wget. To do this, we use the zip function, which ABSOLUTELY DEPENDS on the consistent indexing we have just established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look great! Now, to use the os library's wget function, we need to manually change directory just as we would in bash/ UNIX. We also need to track which directory we are in and create a folder in which to store the data for each charter.\n",
    "#### Students: be sure to change the directory listed in these commands to one that works for you! Otherwise it will throw an error, and the wget command below won't work. ####\n",
    "\n",
    "To start, we cd into the data directory, and then us getcwd (the equivalent of pwd in bash) to check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anhnguyen/Desktop/research/scraping_Python/wget_20'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(wget_folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Wget commands\n",
    "\n",
    "Note:the trailing slash on the URL is critical – if you omit it, wget will think that papers is a file rather than a directory. x/abc/ means downloading entire x/abc/ page while x/abc means downloading abc file\n",
    "\n",
    "-r recursively download files. it will also follow any other links: if there was a link to somewhere on that page, it would follow that and download it as well. By default, -r sends wget to a depth of five sites after the first one. \n",
    "\n",
    "--no-parent wget should follow links, but not beyond the last parent directory\n",
    "\n",
    "-O  Download and Store With a Different File name\n",
    "\n",
    "-b For a huge download, put the download in background\n",
    "\n",
    "--user-agent mask the user agent by using –user-agent options and show wget like a browser\n",
    "\n",
    "--tries increase retry attempts. Default: 20 times\n",
    "\n",
    "-i Download Multiple Files / URLs. First, need to create a urls.txt file.\n",
    "\n",
    "--mirror download a full website and made available for local viewing\n",
    "\n",
    "-r -A Download only certain file types\n",
    "\n",
    "-e robots=off execute command and ignore robots meta tags and robots.txt\n",
    "\n",
    "--spider determine weather the remote file exist at the destination \n",
    "\n",
    "--domains download only only PDF files from specific domains\n",
    "\n",
    "--user --password download files from password protected sites \n",
    "(wget --user=user --password=password http://www.example.com)\n",
    "\n",
    "--http-user --http-password Download from password protected http sites\n",
    "\n",
    "credits\n",
    "http://programminghistorian.org/lessons/automated-downloading-with-wget\n",
    "https://www.gnu.org/software/wget/manual/wget.html\n",
    "http://www.techsakh.com/2015/11/06/wget-examples-in-linux/\n",
    "http://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(learning_wget)\n",
    "\n",
    "# try different wgets, for learning purposes only\n",
    "#note \n",
    "os.system('wget -m -w 2 --limit-rate=20k http://activehistory.ca')\n",
    "os.system('wget -r -A.pdf http://url-to-webpage-with-pdfs/')\n",
    "os.system('wget -O taglist.zip http://www.vim.org/scripts/download_script.php?src_id=7701')\n",
    "os.system('wget --user-agent=\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3\" http://activehistory.ca')\n",
    "os.system('wget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com')\n",
    "# os.system('wget -O --secure-protocol=auto --no-check-certificate --execute robots=off http://www.berkeley.edu/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now we've got tuples and we're in the right directory to start with. We are finally ready to run wget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing website data for Richland Two Charter High, which is school #1 of 300...\n",
      "Capturing website data for Polk State College Collegiate High School, which is school #2 of 300...\n",
      "Capturing website data for River City Scholars Charter Academy, which is school #3 of 300...\n",
      "Capturing website data for Detroit Enterprise Academy, which is school #4 of 300...\n",
      "Capturing website data for Lighthouse Community Sch Inc, which is school #5 of 300...\n",
      "Capturing website data for Westlake Charter Middle, which is school #6 of 300...\n",
      "Capturing website data for Van Gogh Charter, which is school #7 of 300...\n",
      "Capturing website data for Summit Academy Transition High School Dayton, which is school #8 of 300...\n",
      "Capturing website data for Westchester Academy For International Studies, which is school #9 of 300...\n",
      "Capturing website data for City Academy, which is school #10 of 300...\n",
      "Capturing website data for Byrneville Elementary School, Inc., which is school #11 of 300...\n",
      "Capturing website data for Acacia Middle Charter, which is school #12 of 300...\n",
      "Capturing website data for Whittier Elementary, which is school #13 of 300...\n",
      "Capturing website data for Zoe Learning Acad - Ambassador Campus, which is school #14 of 300...\n",
      "Capturing website data for Young Womens Leadership Chartr Hs, which is school #15 of 300...\n",
      "Capturing website data for Yuba County Career Preparatory Charter, which is school #16 of 300...\n",
      "Capturing website data for Young Scholars Of Western Pennsylvania C, which is school #17 of 300...\n",
      "Capturing website data for West Phila. Achievement Ces, which is school #18 of 300...\n",
      "Capturing website data for John H Wood Jr Cs Granbury, which is school #19 of 300...\n",
      "Capturing website data for Willow Creek Charter School, which is school #20 of 300...\n",
      "Capturing website data for White Pine Academy, which is school #21 of 300...\n",
      "Capturing website data for West Ridge Academy, which is school #22 of 300...\n",
      "Capturing website data for Watts Learning Center Charter Middle, which is school #23 of 300...\n",
      "Capturing website data for Waterford Montessori Academy, which is school #24 of 300...\n",
      "Capturing website data for Valley Life Charter, which is school #25 of 300...\n",
      "Capturing website data for Valley Charter Elementary, which is school #26 of 300...\n",
      "Capturing website data for Virtual Academy Of Lafourche, which is school #27 of 300...\n",
      "Capturing website data for Urban Prep Chtr Acad Bronzeville Hs, which is school #28 of 300...\n",
      "Capturing website data for Two Rivers Community School, which is school #29 of 300...\n",
      "Capturing website data for Two Dimensions/Vickery, which is school #30 of 300...\n",
      "Capturing website data for Trinity School For Children, which is school #31 of 300...\n",
      "Capturing website data for Texas Leadership Of Midland, which is school #32 of 300...\n",
      "Capturing website data for Taos Integrated School Of Arts, which is school #33 of 300...\n",
      "Capturing website data for Cincinnati State Stem Academy, which is school #34 of 300...\n",
      "Capturing website data for Mana Academy Charter School, which is school #35 of 300...\n",
      "Capturing website data for The Green School, which is school #36 of 300...\n",
      "Capturing website data for The Education Center In Lewisville, which is school #37 of 300...\n",
      "Capturing website data for Thea Bowman Leadership Academy, which is school #38 of 300...\n",
      "Capturing website data for Fort Worth Can Academy - South, which is school #39 of 300...\n",
      "Capturing website data for Dallas Can Academy Charter, which is school #40 of 300...\n",
      "Capturing website data for Telesis Preparatory, which is school #41 of 300...\n",
      "Capturing website data for Ethical Community Charter School (The), which is school #42 of 300...\n",
      "Capturing website data for Tapestry Charter School, which is school #43 of 300...\n",
      "Capturing website data for Trinity Academy Performing Art, which is school #44 of 300...\n",
      "Capturing website data for Southwest Schools Mangum El Campus, which is school #45 of 300...\n",
      "Capturing website data for Southwest Preparatory School-Northwest, which is school #46 of 300...\n",
      "Capturing website data for Sun Valley Charter School, which is school #47 of 300...\n",
      "Capturing website data for Sunny Wolf Charter School, which is school #48 of 300...\n",
      "Capturing website data for Suncoast School For Innovative Studies, which is school #49 of 300...\n",
      "Capturing website data for Kaizen Education Foundation Dba Summit High School, which is school #50 of 300...\n",
      "Capturing website data for Strive Prep - Smart Academy, which is school #51 of 300...\n",
      "Capturing website data for St Hope Leadership Academy Charter School, which is school #52 of 300...\n",
      "Capturing website data for St Croix Preparatory Academy Middle, which is school #53 of 300...\n",
      "Capturing website data for Stony Point Academy, which is school #54 of 300...\n",
      "Capturing website data for Southside Academy, which is school #55 of 300...\n",
      "Capturing website data for South Pointe High School, which is school #56 of 300...\n",
      "Capturing website data for South Bronx Classical Charter School Ii, which is school #57 of 300...\n",
      "Capturing website data for Somerset Preparatory Academy Pcs, which is school #58 of 300...\n",
      "Capturing website data for Somerset Academy Charter High, which is school #59 of 300...\n",
      "Capturing website data for Southwest Leadership Academy Cs, which is school #60 of 300...\n",
      "Capturing website data for Alliance Cindy And Bill Simon Technology Academy H, which is school #61 of 300...\n",
      "Capturing website data for School For Integrated Academics And Technologies, which is school #62 of 300...\n",
      "Capturing website data for Shiloh Charter, which is school #63 of 300...\n",
      "Capturing website data for Leads Primary Charter School, which is school #64 of 300...\n",
      "Capturing website data for Sequoia Village School, which is school #65 of 300...\n",
      "Capturing website data for Seed Pcs Of Washington Dc, which is school #66 of 300...\n",
      "Capturing website data for San Diego Cooperative Charter, which is school #67 of 300...\n",
      "Capturing website data for Sauvie Island Academy, which is school #68 of 300...\n",
      "Capturing website data for Sanger Academy Charter, which is school #69 of 300...\n",
      "Capturing website data for Sage Academy, which is school #70 of 300...\n",
      "Capturing website data for River Valley Charter, which is school #71 of 300...\n",
      "Capturing website data for Riverside Academy, which is school #72 of 300...\n",
      "Capturing website data for River City Science Academy Innovation School, which is school #73 of 300...\n",
      "Capturing website data for Richard Allen Leadership Academy, which is school #74 of 300...\n",
      "Capturing website data for Renaissance Charter School At Tradition, which is school #75 of 300...\n",
      "Capturing website data for Relevant Academy Of Eaton County, which is school #76 of 300...\n",
      "Capturing website data for Rapoport Academy El - North Campus, which is school #77 of 300...\n",
      "Capturing website data for Quest Charter School Academy, which is school #78 of 300...\n",
      "Capturing website data for Pioneer Valley Chinese Immersion Charter School, which is school #79 of 300...\n",
      "Capturing website data for Putnam Academy Of Arts And Sciences, which is school #80 of 300...\n",
      "Capturing website data for Providence Hall, which is school #81 of 300...\n",
      "Capturing website data for Camden'S Pride Charter School, which is school #82 of 300...\n",
      "Capturing website data for Prestige Academy, which is school #83 of 300...\n",
      "Capturing website data for Portage Academy Of Achievement, which is school #84 of 300...\n",
      "Capturing website data for Berkley Accelerated Middle School, which is school #85 of 300...\n",
      "Capturing website data for Pickerington Community School, which is school #86 of 300...\n",
      "Capturing website data for Columbus Performance Academy, which is school #87 of 300...\n"
     ]
    }
   ],
   "source": [
    "k=0 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "\n",
    "#testing the first 10 tuples\n",
    "tuple_test = tuple_list[:10]\n",
    "\n",
    "for tup in tuple_list:\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", (tup[1].title()) + \", which is school #\" + str(k), \"of 300...\")\n",
    "    # use the tuple to create a name for the folder\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + (tup[1].title())\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + (tup[1].title())\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + (tup[1].title())\n",
    "    os.chdir(wget_folder)  # cd into data directory--this is a/\n",
    "    if not os.path.exists(dirname):\n",
    "    # key step because otherwise wget puts the output into whatever folder it was previously in, can get real messy.\n",
    "        os.makedirs(dirname) # create the folder using the dirname we just made, then change into that directory\n",
    "    os.chdir(dirname)  # other options to think about for wget: --page-requisites --retry-connrefused --convert-links --wait=3\n",
    "    \n",
    "    #trying different wgets\n",
    "    if(False):\n",
    "        os.system('wget -i -r -m --level=3 --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png, .PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "         .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX ' +(tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget -pn --mirror --random-wait  -A '+  (tup[0]))\n",
    "    if(True):\n",
    "        os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "          --accept .html' + ' ' + (tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "        --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "        --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png,.PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "        .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX' + ' ' + (tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting wget: what if there's no directory hierarchy?\n",
    "Although it's not common practice, some sites have no directory hierarchy, and this makes it difficult to scrape them automatically. For instance, Chinook Montessori Charter School's home URL is 'http://www.k12northstar.org/chinook', and if you go to this site you'll find links to other website pages with URLs that are NOT nested within this home URL, e.g. 'http://www.k12northstar.org/Page/2684' and 'http://www.k12northstar.org/Page/2685'. \n",
    "\n",
    "Our problem with directory-less URL structures is that the coding strategy used above really depends on wget's \"recursive\" option, which scrapes the site you give it as well as all sites included within that. For instance, it would scrape not only www.school.com/ but also www.school.com/cafeteria and www.school.com/about_us. This won't work for sites with no directory hierarchy. It may be impossible to separate out the site pages for only the school of interest if the site directory includes multiple schools at a \"flat\" level with no hierarchy... But we won't know until we try!\n",
    "\n",
    "Let's look at coding an automated solution to deal with this problem. Thus far, I've only thought up a painstaking solution that involves feeding wget each site manually. Below is an example of that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grabbing #142--a school that has no directory hierarchy in its website:\u001f\u001f\n",
    "os.chdir(no_dir_folder)\n",
    "\n",
    "for url in ['http://www.k12northstar.org/chinook', 'http://www.k12northstar.org/site/Default.aspx?PageID=2678', \\\n",
    "'http://www.k12northstar.org/Page/2684', 'http://www.k12northstar.org/Page/2685', \\\n",
    "'http://www.k12northstar.org/Page/2686', 'http://www.k12northstar.org/Page/2683', \\\n",
    "'http://www.k12northstar.org/Page/2704']:\n",
    "    os.system('wget --no-parent --show-progress --progress=dot --convert-links \\\n",
    "        --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "         --accept .html' + ' ' + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function for getting parents' links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:\n",
    "- run wget on URL L\n",
    "- if wget on L captured no HTML:\n",
    "    - if L has a parent that's a valid URL (call it P):\n",
    "        - wget on P, set k=1\n",
    "    - if wget on P captured no HTML:\n",
    "        - if P has a parent that's a valid URL (call it D):\n",
    "        - wget on D, set k=2\n",
    "    - if wget on D captured HTML, repeat again (total 5 nested ifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not needed for wget\n",
    "# just tested things out, this method might be useful?\n",
    "def get_parent_link(str):\n",
    "    \"\"\"Function to get parents' links. Return a list of valid links.\"\"\"\n",
    "    return get_parent_link_helper(3, str, []);\n",
    "\n",
    "def get_parent_link_helper(level, str, result):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to get parent link of a given link. Return a list of urls \"\"\"\n",
    "    if level == 0 or not check(str):\n",
    "        return result\n",
    "    else:\n",
    "        result += [str]\n",
    "        return get_parent_link_helper(num -1, str[: str.rindex('/')], result)\n",
    "\n",
    "#needed for wget\n",
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        return True\n",
    "    except urllib.error.URLError:\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.k12northstar.org/chinook', 'http://www.k12northstar.org']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parent_link(\"http://www.k12northstar.org/chinook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified version for wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wget(str):\n",
    "    \"\"\"wget a link or its parent link. Default: 5 levels\"\"\"\n",
    "    return wget_helper(5, str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/Users/anhnguyen/Desktop/research/scraping_Python/parent_lins_tests\"\n",
    "def wget_helper(level, str):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to wget on url and its parents \"\"\"\n",
    "    \n",
    "    if level == 0 or wget_success(level, str):\n",
    "        return\n",
    "    else:\n",
    "        return wget_helper(level -1, str[: str.rindex('/')])\n",
    "    \n",
    "def wget_success(level, link):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "    #check if the link exists\n",
    "    if not check(link):\n",
    "        return False\n",
    "    #create folder, ready for wget\n",
    "    os.chdir(path)\n",
    "    folder_name = str(level) +\"_\"+ link[12:]\n",
    "    os.makedirs(folder_name)\n",
    "    os.chdir(folder_name)\n",
    "    os.system('wget {}'.format(link))\n",
    "    \n",
    "    #did wget get anything?\n",
    "    list_of_files = os.listdir(path + '/'+folder_name)\n",
    "    if list_of_files == []:\n",
    "        return False\n",
    "    \n",
    "    #is there a html file\n",
    "    if (True in [file_name.endswith('.html') for file_name in list_of_files]):\n",
    "        return True\n",
    "    return False     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wget('http://www.k12northstar.org/chinook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing to see how many folders has a html file\n",
    "\n",
    "#customized function from wget_success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
