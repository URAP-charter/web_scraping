{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLLIB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# import necessary libraries\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function\n",
    "\n",
    "def contains_html(my_folder):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith('.html'):\n",
    "                return True\n",
    "    return False  \n",
    "def count_with_file_ext(folder, ext):\n",
    "    count = 0\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith(ext):\n",
    "                count +=1\n",
    "    return count  \n",
    "\n",
    "def write_file(str, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "        \n",
    "def reset(folder, text_file_1, text_file_2):\n",
    "    \"\"\"Deletes all files in a folder and set 2 text files to blank\"\"\"\n",
    "    parent_folder = folder[: folder.rindex('/')]\n",
    "    shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    filelist = [ f for f in os.listdir(folder) if f.endswith(\".bak\") ]\n",
    "    for f in filelist:\n",
    "        os.unlink(f)\n",
    "    for file_name in [text_file_1, text_file_2]:\n",
    "        reset_text_file(file_name)\n",
    "        \n",
    "def reset_text_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(\"\")\n",
    "\n",
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        return False\n",
    "    except SocketError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def read_txt(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:     \n",
    "            count +=1\n",
    "            links += [line.rstrip()]\n",
    "    return links, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_file= '/Users/anhnguyen/Desktop/research/scraping_Python/valid_links.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urlopen\n",
    "Get html files from a link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n",
      "b'\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html __expr-val-dir=\"ltr\" lang=\"en-us\" dir=\"ltr\">\\r\\n<head><meta charset=\"utf-8\" /><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" /><meta name=\"viewport\" content=\"'\n"
     ]
    }
   ],
   "source": [
    "# Option 1:\n",
    "html = urlopen(\"https://www.nhaschools.com/schools/detroitenterprise\")\n",
    "print(html.read()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html>\\n<!--[if lt IE 7]>   <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\">   <![endif]-->\\n<!--[if IE 7]>      <html class=\"no-js ie7 lt-ie8 lt-ie9\">          <![endif]-->\\n<!--[if IE 8]>      <h'\n"
     ]
    }
   ],
   "source": [
    "# Option 2:\n",
    "with urlopen('http://python.org/') as response:\n",
    "   html = response.read()\n",
    "print(html[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "#run urllip on valid links that wget didn't capture\n",
    "links, count = read_txt(link_file)\n",
    "print(count)\n",
    "def check_with_urlopen(links):\n",
    "    success = 0\n",
    "    for link in links:\n",
    "        with urlopen('http://python.org/') as response:\n",
    "            if response.read()[:200] != \"\":\n",
    "                success += 1\n",
    "    return success\n",
    "\n",
    "count_success = check_with_urlopen(links)\n",
    "print(count_success)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cool! It works on all 37 links that wget failed to capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urlretrieve\n",
    "Retrieve a resource via URL and store it in a temporary location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/anhnguyen/Desktop/research/scraping_Python/other_tools/test_file.txt',\n",
       " <http.client.HTTPMessage at 0x104941f28>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = '/Users/anhnguyen/Desktop/research/scraping_Python/other_tools/test_file.txt'\n",
    "urllib.request.urlretrieve(\"https://www.nhaschools.com/schools/detroitenterprise\",test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'grabber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4cb2c05fdcc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgrabber\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlgrab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlocal_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlgrab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# grab a local copy of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# just read the data into a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'grabber'"
     ]
    }
   ],
   "source": [
    "### urlgrabber\n",
    "from grabber import urlgrab\n",
    "local_filename = urlgrab(url)  # grab a local copy of the file\n",
    "data = urlread(url)            # just read the data into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits\n",
    "<ul>\n",
    "    <li>https://docs.python.org/2/library/urllib.html</li>\n",
    "    <li>https://docs.python.org/3.4/howto/urllib2.html</li>\n",
    "    <li>http://urlgrabber.baseurl.org/help/urlgrabber.grabber.html</li>\n",
    "    <li>http://urlgrabber.baseurl.org/examples.html</li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
