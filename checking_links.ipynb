{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid URL and URLLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno\n",
    "import pandas as pd\n",
    "import httplib2\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import requests\n",
    "from requests.auth import HTTPDigestAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/excel_files/micro-sample_MANUAL_Apr17.xlsx\"\n",
    "valid = \"/Users/anhnguyen/Desktop/research/scraping_Python/text_files/valid_march.txt\"\n",
    "invalid = \"/Users/anhnguyen/Desktop/research/scraping_Python/text_files/invalid_march.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Panda to load the excel file. We are interested in only url column for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(micro_sample_cvs)\n",
    "urls = df[\"URL\"]\n",
    "school_names = df[\"School name\"]\n",
    "# print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list <- our backup plan\n",
    "    This functions helps to check the url that has service unavailable issues\n",
    "    Since status code fails to check this.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        print(url + \" :URLError\")\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        print(url +' :HTTPError')\n",
    "        return False\n",
    "    except SocketError:\n",
    "        print(url + 'SocketError')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def check_url(url):\n",
    "    \"\"\"This functions uses the status code to determine if the link is valid. This resolves\n",
    "    the links that redirects and most cases of authentication problems\"\"\"\n",
    "    code = \"[no code collected]\"\n",
    "    if url == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        r = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "        code = r.status_code\n",
    "        #backup plan for service unavailable issues\n",
    "        if code == 503:\n",
    "            return check(url)\n",
    "        if code < 400:\n",
    "            return True   \n",
    "    except:\n",
    "        pass\n",
    "    print(str(url) +\" ---Error code: \" + str(code))\n",
    "    return False    \n",
    "\n",
    "# just write str to file\n",
    "def write_file(str, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "def reset_text_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(\"\")\n",
    "                \n",
    "def count_valid_links(list_of_names,list_of_links, valid_file, invalid_file):\n",
    "    \"\"\"we want to determine how many valid and invalid links\"\"\"\n",
    "    count_success, count_fail = 0, 0\n",
    "    valid, invalid = '', ''\n",
    "    for i in range(0,len(list_of_links)):\n",
    "        if check_url(list_of_links[i]):\n",
    "            valid += str(list_of_names[i]) +\"\\t\" +list_of_links[i]+ '\\n'\n",
    "            count_success +=1\n",
    "        else:\n",
    "            invalid += str(list_of_links[i]) + '\\n'\n",
    "            count_fail += 1\n",
    "    write_file(valid, valid_file)\n",
    "    write_file(invalid, invalid_file)\n",
    "    return count_success, count_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan ---Error code: [no code collected]\n",
      "nan ---Error code: [no code collected]\n",
      "nan ---Error code: [no code collected]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "reset_text_file(invalid)\n",
    "reset_text_file(valid)\n",
    "yes, no =count_valid_links(school_names, urls, valid, invalid)\n",
    "print(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 15\n"
     ]
    }
   ],
   "source": [
    "check_url(\"http://www.appletonpublicmontessori.com/\")\n",
    "print(yes, no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### There are 16 invalid links and the rest are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(\"http://www.appletonpublicmontessori.com/\")\n",
    "# for link in soup.findAll('a', attrs={'href': re.compile(\"((http|ftp)s?://.*?)\")}):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# html_page = requests.get(\"http://www.wattslearningcenter.org/\")\n",
    "# soup = BeautifulSoup(html_page.text)\n",
    "# count = 0;\n",
    "\n",
    "# for link in soup.findAll('a'):\n",
    "#     pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "#     url_parent = \"http://www.wattslearningcenter.org/\"\n",
    "#     count +=1\n",
    "#     current_link = link.get('href')\n",
    "#     if not pattern.match(current_link):\n",
    "#         current_link = urljoin(url_parent, current_link)\n",
    "#     print(current_link)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
