{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import httplib2, requests, contextlib\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from requests.auth import HTTPBasicAuth, HTTPDigestAuth\n",
    "from os.path import splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: csv file here\n",
    "micro_sample_cvs = \"micro-sample_MANUAL_Apr17.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(str, file_name):\n",
    "    \"\"\"This function writes a string str to a given file name\"\"\"\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "def save_json(file, school):\n",
    "    with open(file, 'wb') as outfile:\n",
    "#         json.dump(school, outfile)\n",
    "        pickle.dump(school, outfile)\n",
    "#want the obj back\n",
    "def get_obj_back(file):\n",
    "    with open(file,'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list <- our backup plan\n",
    "    This functions helps to check the url that has service unavailable issues\n",
    "    Since status code fails to check this.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        print(url + \" :URLError\")\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        print(url +' :HTTPError')\n",
    "        return False\n",
    "    except SocketError:\n",
    "        print(url + 'SocketError')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def check_url(url):\n",
    "    \"\"\"This functions uses the status code to determine if the link is valid. This resolves\n",
    "    the links that redirects and most cases of authentication problems\"\"\"\n",
    "    code = \"[no code collected]\"\n",
    "    if url == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        r = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "        code = r.status_code\n",
    "        #backup plan for service unavailable issues\n",
    "        if code == 503:\n",
    "            return check(url)\n",
    "        if code < 400:\n",
    "            return True   \n",
    "    except:\n",
    "        pass\n",
    "    print(\"Encounter an invalid link: \" + str(url) +\" ---Error code: \" + str(code))\n",
    "    return False    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(micro_sample_cvs)\n",
    "school_urls = df[\"URL\"]\n",
    "school_names = df[\"School name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the links of a websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_children_links(url_parent, hostname, visited, depth, useless):\n",
    "    \"\"\"This function recursively gets the children links of a given links\"\"\"\n",
    "    #we have gone through enough levels or visited this link already \n",
    "    if depth == 0 or url_parent in visited or url_parent in useless:\n",
    "        return set()\n",
    "    if not check_url(url_parent):\n",
    "        useless.add(url_parent)\n",
    "        return set()\n",
    "    \n",
    "    #get the html page\n",
    "    #parse into a BS object\n",
    "    html_page = requests.get(url_parent, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html_page.text)\n",
    "\n",
    "    #we visited url_parent, updated into the set\n",
    "    visited.add(url_parent)\n",
    "    \n",
    "    #now checking its children\n",
    "    for link in soup.findAll('a'):\n",
    "        #running recursively in a try-except block to prevent broken links break the code\n",
    "        try:\n",
    "            pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "            current_link = link.get('href')\n",
    "#             print(current_link)\n",
    "            if not pattern.match(current_link):\n",
    "                current_link = urljoin(url_parent, current_link)\n",
    "            \n",
    "            #check if the link is within the domain (hostname)\n",
    "            if hostname in current_link:\n",
    "#                 print(current_link)\n",
    "                #combine results from its children's links\n",
    "                get_children_links(current_link, hostname, visited, depth -1, useless)\n",
    "        except:\n",
    "            pass\n",
    "    return visited\n",
    "#     print(count)\n",
    "\n",
    "def getLinks(url, depth):\n",
    "    text,useless = set(), set()\n",
    "    hostname = urlparse(url).hostname\n",
    "    return get_children_links(url, hostname, text, depth, useless)\n",
    "#delete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter an invalid link: https://www.richland2.org/~ ---Error code: 404\n",
      "Runing test\n",
      "Here are all the links from: https://www.richland2.org/charterhigh/\n",
      "https://www.richland2.org/CharterHigh/School-Information/Graduation-2017\n",
      "https://www.richland2.org/CharterHigh/Careers\n",
      "https://www.richland2.org/accessibility.aspx\n",
      "https://www.richland2.org/CharterHigh/School-Information/Directions\n",
      "https://www.richland2.org/CharterHigh/News/Welcome-to-Our-New-Website!\n",
      "https://www.richland2.org/CharterHigh/School-Information\n",
      "https://www.richland2.org/charterhigh/About-Our-School/Calendar\n",
      "https://www.richland2.org/CharterHigh/News/We-re-Hiring-a-Principal\n",
      "https://www.richland2.org/charterhigh/news\n",
      "https://www.richland2.org/CharterHigh/news\n",
      "https://www.richland2.org/CharterHigh/School-Information/School-Board\n",
      "https://www.richland2.org/CharterHigh/School-Information/Calendar\n",
      "https://www.richland2.org/CharterHigh/School-Information/Lab-Hours\n",
      "https://www.richland2.org/CharterHigh/Student-Registration\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "https://www.richland2.org/CharterHigh/\n",
      "https://www.richland2.org/CharterHigh/School-Information/Frequently-Asked-Questions\n",
      "https://www.richland2.org/CharterHigh/News\n",
      "https://www.richland2.org/CharterHigh/About-Us\n",
      "https://www.richland2.org/CharterHigh/Special-Pages/Site-Translations\n",
      "https://www.richland2.org/CharterHigh/Contact-Us\n",
      "https://www.richland2.org/charterhigh/\n",
      "https://www.richland2.org/CharterHigh/School-Information/School-Improvement-Council\n",
      "https://www.richland2.org\n",
      "There are 24 of them\n"
     ]
    }
   ],
   "source": [
    "# test on 1 url\n",
    "urls = getLinks(\"https://www.richland2.org/charterhigh/\", 5)\n",
    "count = 0\n",
    "print(\"Runing test\")\n",
    "print(\"Here are all the links from: \" + \"https://www.richland2.org/charterhigh/\" )\n",
    "for e in urls:\n",
    "    count += 1\n",
    "    print(e)\n",
    "print(\"There are \" + str(count)+ \" of them\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run this method on 300 sites, save as dictionary and store it as a JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schools = dict()\n",
    "cap = len(school_names)\n",
    "# cap = 2\n",
    "for i in range(cap):\n",
    "    link, name = school_urls[i], school_names[i]\n",
    "    #sanity check make sure all links are valid\n",
    "    if not check_url(link):\n",
    "        print(\"Hey! \" + name + \" doesnt have a valid link! \" + link)\n",
    "    print(school_names[i])\n",
    "    #saving school names as keys and list of links as values\n",
    "    schools.update({name: list(getLinks(link, 4))})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_json(\"list_of_links\", schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "school_list = get_obj_back(\"list_of_links\")\n",
    "for i in school_list:\n",
    "    print(i + \" has \" + str(len(school_list[i])) )\n",
    "    print(school_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get all the html text by runing requests to get all the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get all images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we all have a list of the links alreay, now we are ready to create folders to store these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_images(url):\n",
    "\n",
    "    html_page = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "    images = set()\n",
    "    a = list()\n",
    "#     for s in soup.findAll('div'):\n",
    "#         a += s.findAll(\"img\")\n",
    "    a += list(soup.select('img'))\n",
    "    for link in a:\n",
    "        try:\n",
    "            image = link.get(\"src\")          \n",
    "            image = abs_link(url,image)\n",
    "            images.add(image)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return images\n",
    "\n",
    "def abs_link(url_parent,image):\n",
    "    \"\"\"returns the absolute link of a given url\"\"\"\n",
    "    pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "    #relative link, need to append to parent's link to that the correct link\n",
    "    if not pattern.match(image):\n",
    "        image = urljoin(url_parent, image)\n",
    "    return image\n",
    "\n",
    "def download_image(url, k):\n",
    "    try:\n",
    "        response = requests.get(url,stream=True, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "        ext = url[url.rindex(\".\"):]\n",
    "        with open(\"img\" + str(k)+ ext, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "            del response\n",
    "    except:\n",
    "        print(\"couldn't get \" + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://static.politico.com/dims4/default/ed5167d/2147483647/resize/1160x%3E/quality/90/?url=http%3A%2F%2Fs3-origin-images.politico.com%2F2013%2F09%2F29%2Fmccarthy_blackburn_cruz_ap_ftn_ap_328.jpg\n",
      "data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\n",
      "couldn't get data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\n"
     ]
    }
   ],
   "source": [
    "r  = get_images(\"http://www.politico.com/story/2013/09/government-shutdown-2013-gop-narrative-97521.html\")\n",
    "k = 1\n",
    "os.chdir(\"/Users/anhnguyen/Desktop/research/scraping_Python/tex2\")\n",
    "for i in r:\n",
    "    print(i)\n",
    "    download_image(i, k)\n",
    "    k += 1\n",
    "#might need to check on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://s3-origin-images.politico.com/2013/09/29/mccarthy_blackburn_cruz_ap_ftn_ap_328.jpg\n"
     ]
    }
   ],
   "source": [
    "#scrap preview images, but not sure if we need those\n",
    "url = \"http://www.politico.com/story/2013/09/government-shutdown-2013-gop-narrative-97521.html\"\n",
    "html_page = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "soup = BeautifulSoup(html_page.text, \"lxml\")\n",
    "\n",
    "metatag = soup.find(\"meta\", {\"property\": \"og:image\"})\n",
    "if metatag is not None:\n",
    "    print(metatag[\"content\"])\n",
    "else:\n",
    "    print(\"This page has no Open Graph meta image tag\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pdfs(url):\n",
    "\n",
    "    html_page = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "    soup = BeautifulSoup(html_page.text, \"lxml\")\n",
    "    pdfs = set()\n",
    "    # getting the links of the images\n",
    "    for link in soup.findAll('a'):\n",
    "        try:\n",
    "            pdf = link.get('href')          \n",
    "            pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "            #relative link, need to append to parent's link to that the correct link\n",
    "            if not pattern.match(pdf):\n",
    "                pdf = urljoin(url, pdf)\n",
    "                \n",
    "            if(pdf.endswith(\".pdf\")):\n",
    "                pdfs.add(pdf)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pdfs \n",
    "def download_pdf(url):\n",
    "    try:\n",
    "        \n",
    "        file_name = url[url.rindex(\"/\")+1: url.rindex(\".pdf\")] +\".pdf\"\n",
    "#         print(file_name)\n",
    "        if os.path.exists(file_name):\n",
    "            file_name = file_name[:file_name.rindex(\".pdf\")] +\"_1.pdf\"\n",
    "        response = requests.get(url,stream=True, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except:\n",
    "        print(\"couldn't get \" + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m  = get_pdfs(\"http://inst.eecs.berkeley.edu/~ee16b/sp17/\")\n",
    "os.chdir(\"/Users/anhnguyen/Desktop/research/scraping_Python/tex2\")\n",
    "for i in m:\n",
    "    print(i)\n",
    "    download_pdf(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is the fun part. We will run our code to get everything, from text, images, and pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_folder_name (k, name):\n",
    "    \"\"\"Format a folder nicely for easy access\"\"\"\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + name\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + name\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + name\n",
    "    return dirname\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_master(school_list, parent_folder):\n",
    "    \"\"\"webscrap links and print output to appropriate folders\"\"\"\n",
    "    #navigate to parent folder\n",
    "    k = 0\n",
    "    os.chdir(parent_folder)\n",
    "    for k, school in enumerate(school_list):\n",
    "        # create dir my_folder if it doesn't exist yet\n",
    "        os.chdir(parent_folder)\n",
    "        curr_folder_name = format_folder_name(k, school)\n",
    "        if not os.path.exists(curr_folder_name):\n",
    "            os.makedirs(curr_folder_name)\n",
    "        #navigate to the correct folder, ready to scrap\n",
    "        os.chdir(curr_folder_name)\n",
    "        \n",
    "        #scrap link to get html content\n",
    "        for url in school_list[school]:\n",
    "            html_page = requests.get(url, auth=HTTPDigestAuth('user', 'pass'), headers= {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"})\n",
    "            soup = BeautifulSoup(html_page.text)\n",
    "            try:\n",
    "                #beautify text name for easy access, accept alphanumeric letters only\n",
    "                #name is the title of the page\n",
    "                file_name = re.sub('[^A-Za-z0-9]+', '', soup.title.string)\n",
    "            except:\n",
    "                file_name = \"no name\"\n",
    "#                 i += 1\n",
    "            #create folder if not exists & go to that folder\n",
    "            if not os.path.exists(file_name):\n",
    "                os.makedirs(file_name)\n",
    "            os.chdir(file_name)\n",
    "            \n",
    "            #format text as follow \n",
    "            #first line: url, second line :-------------(separater), third line - end: scraping result\n",
    "            write_file(\"URL:\" + str(url)+ \"\\n\",\"text.txt\" )\n",
    "            write_file(\"----------------------------------------------\\n\", \"text.txt\")\n",
    "            write_file(str(soup.prettify()), \"text.txt\")\n",
    "            \n",
    "            #now get all the images\n",
    "            list_images = get_images(url)\n",
    "            for num,img in enumerate(list_images):\n",
    "                download_image(img, num)\n",
    "            #now get all the pdfs\n",
    "            list_pdf = get_pdfs(url)  \n",
    "            for pdf in list_pdf:\n",
    "                download_pdf(pdf)\n",
    "                \n",
    "            #done, go back to parent directory, ready to scrape the next url\n",
    "            os.chdir('..')\n",
    "        \n",
    "#         k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/python requests results\"\n",
    "run_master(school_list, parent_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\tAbout Us\r\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
