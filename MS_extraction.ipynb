{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import os, csv\n",
    "# other options:   import os.system(wget http)   OR   import urllib.urlretrieve (this is an alternative command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/no_dir\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 OLD CLEMSON RD, COLUMBIA, SC \n",
      " https://www.richland2.org/charterhigh/ \n",
      " {'HPALM': '0', 'ASPKF': '-2', 'AS08M': '-2', 'WH02F': '-2', 'REDLCH': '4', 'AM03F': '-2', 'PK': '-2', 'ASKGM': '-2', 'TRALM': '1', 'LCITY': 'COLUMBIA', 'AMUGF': '-2', 'WH06F': '-2', 'ISMEMPUP': 'PS', 'CHARTR': '1', 'WH09M': '0', 'HIPKF': '-2', 'BLALF': '23', 'STID': '4002', 'SEARCH PARAMS': 'RICHLAND TWO CHARTER HIGH 750 OLD CLEMSON RD, COLUMBIA, SC', 'AMALF': '0', 'TR12F': '0', 'HP08M': '-2', 'BL10F': '3', 'KGOFFRD': '2', 'TR05M': '-2', 'AM01M': '-2', 'MAGNET': '2', 'RECONSTF': '2', 'LSTATE': 'SC', 'HI10M': '1', 'HP11F': '0', 'WH06M': '-2', 'TR': '3', 'RDM #': '0.934345634', 'WHITE': '21', 'ISFLE': 'PS', 'WH04F': '-2', 'HP09M': '0', 'TR10M': '0', 'AM09M': '0', 'RECONSTY': 'N', 'BL12F': '10', 'HI03F': '-2', 'AS02M': '-2', 'G05OFFRD': '2', 'TR07F': '-2', 'NCESSCH': '4.50E+11', 'HP01F': '-2', 'AS05M': '-2', 'URL': 'https://www.richland2.org/charterhigh/', 'WAYBACK URL': 'https://web.archive.org/web/*/https://www.richland2.org/charterhigh', 'TR06F': '-2', 'G03OFFRD': '2', 'HP04F': '-2', 'WH03F': '-2', 'TR03F': '-2', 'HI11F': '1', 'HP03F': '-2', 'HIALM': '1', 'TR10F': '0', 'G06': '-2', 'BL01M': '-2', 'G01': '-2', 'CHARTAUTH1': '4002', 'BL04F': '-2', 'HPPKF': '-2', 'MZIP4': '0', 'AM04F': '-2', 'BL11F': '9', 'HP07F': '-2', 'WH07M': '-2', 'TRKGF': '-2', 'HPUGM': '-2', 'G02': '-2', 'MZIP': '29229', 'AMKGM': '-2', 'HI06M': '-2', 'WHPKF': '-2', 'AMUGM': '-2', 'BL06F': '-2', 'G11': '28', 'BL06M': '-2', 'BL12M': '7', 'WHPKM': '-2', 'TR05F': '-2', 'FIPST': '45', 'TITLEI': '2', 'HP09F': '0', 'HI08M': '-2', 'AS03M': '-2', 'AS05F': '-2', 'WHKGM': '-2', 'TR11F': '2', 'AM11M': '0', 'HI06F': '-2', 'WH05F': '-2', 'AM10M': '0', 'HP08F': '-2', 'WH08M': '-2', 'HP12F': '0', 'TOTETH': '69', 'SPWHITE': '2', 'SURVYEAR': '2013', 'HI04F': '-2', 'HI04M': '-2', 'BLALM': '16', 'AMPKF': '-2', 'WH04M': '-2', 'MCITY': 'COLUMBIA', 'HI11M': '0', 'TITLEISTAT': '6', 'G10': '7', 'BL10M': '1', 'AS06M': '-2', 'HP05F': '-2', 'AM11F': '0', 'SFTEPUP': '2', 'WH09F': '1', 'WH01M': '-2', 'TR12M': '1', 'HPALF': '0', 'BLPKM': '-2', 'HP07M': '-2', 'AMKGF': '-2', 'WH10M': '1', 'AM02F': '-2', 'SPELM': '2', 'AS06F': '-2', 'WH01F': '-2', 'HIPKM': '-2', 'WH11F': '6', 'ASALM': '0', 'AM06F': '-2', 'G09OFFRD': '1', 'G10OFFRD': '1', 'CHARTAUTH2': 'M', 'HP10M': '0', 'HI05F': '-2', 'AM': '0', 'WH12F': '6', 'HP02F': '-2', 'ISFTEPUP': 'PS', 'AM10F': '0', 'G01OFFRD': '2', 'HPPKM': '-2', 'G08OFFRD': '2', 'G02OFFRD': '2', 'AS08F': '-2', 'BL08F': '-2', 'AS07F': '-2', 'AM05M': '-2', 'HP06F': '-2', 'WH05M': '-2', 'ISPWHITE': 'PS', 'BL03F': '-2', 'AS07M': '-2', 'WH02M': '-2', 'AMPKM': '-2', 'PACIFIC': '0', 'TR04M': '-2', 'LEVEL': '3', 'AM07M': '-2', 'TRPKF': '-2', 'HI09F': '0', 'AM08M': '-2', 'AS04F': '-2', 'AM07F': '-2', 'NSLPSTATUS': 'M', 'G07': '-2', 'AM06M': '-2', 'GSLO': '9', 'WH12M': '4', 'BL08M': '-2', 'AM03M': '-2', 'G11OFFRD': '1', 'G08': '-2', 'G04': '-2', 'ASUGM': '-2', 'WHKGF': '-2', 'HIALF': '4', 'AS09F': '0', 'HI07M': '-2', 'SFLE': '2', 'G05': '-2', 'AM02M': '-2', 'HI02M': '-2', 'LEANM': 'RICHLAND 02', 'AS03F': '-2', 'FRELCH': '12', 'SEASCH': '600', 'BIES': '2', 'BL09F': '1', 'HP03M': '-2', 'LZIP': '29229', 'BL04M': '-2', 'ISPFEMALE': 'PS', 'BL07F': '-2', 'TR01F': '-2', 'BLUGF': '-2', 'TRUGF': '-2', 'AM09F': '0', 'TYPE': '1', 'HP05M': '-2', 'WHUGF': '-2', 'LEAID': '4503390', 'UNION': '0', 'WH07F': '-2', 'ULOCAL': '21', 'AS10M': '0', 'HIKGM': '-2', 'TR08M': '-2', 'ASALF': '1', 'HI12M': '0', 'TRALF': '2', 'LATCOD': '34.1231', 'STATUS': '1', 'SPFEMALE': '2', 'HI09M': '0', 'TRKGM': '-2', 'WGET INDEX': '1', 'SMEMPUP': '2', 'TR04F': '-2', 'G12OFFRD': '1', 'AM01F': '-2', 'CONUM': '45079', 'TR08F': '-2', 'SCHOOL ID': 'SC600', 'SHARED': '2', 'BLKGF': '-2', 'AM04M': '-2', 'HI03M': '-2', 'BL02M': '-2', 'TR09M': '0', 'CONAME': 'RICHLAND COUNTY', 'ASUGF': '-2', 'ASPKM': '-2', 'WHUGM': '-2', 'SCHNAM': 'RICHLAND TWO CHARTER HIGH', 'BL05M': '-2', 'BL01F': '-2', 'MSTREE': '750 OLD CLEMSON ROAD', 'VIRTUALSTAT': 'VIRTUALNO', 'HI12F': '3', 'AM12F': '0', 'TR11M': '0', 'SCHNO': '1554', 'ASKGF': '-2', 'G03': '-2', 'TR02F': '-2', 'MEMBER': '69', 'MSTATE': 'SC', 'BL02F': '-2', 'HP11M': '0', 'BLUGM': '-2', 'BL03M': '-2', 'WH03M': '-2', 'HIKGF': '-2', 'TR01M': '-2', 'HPKGF': '-2', 'AS12F': '0', 'WH08F': '-2', 'AS11M': '0', 'AM08F': '-2', 'AM05F': '-2', 'HI01F': '-2', 'AS04M': '-2', 'G09': '3', 'BL11M': '7', 'UG': '-2', 'AS10F': '1', 'AS11F': '0', 'HIUGM': '-2', 'ASIAN': '1', 'LZIP4': '0', 'WH10F': '0', 'AS12M': '0', 'HP01M': '-2', 'KG': '-2', 'LSTREE': '750 OLD CLEMSON RD', 'ADDRESS': '750 OLD CLEMSON RD, COLUMBIA, SC', 'HP04M': '-2', 'HP10F': '0', 'TRUGM': '-2', 'HIUGF': '-2', 'WHALM': '8', 'TR06M': '-2', 'HI02F': '-2', 'WH11M': '3', 'PHONE': '8034191348', 'AS09M': '0', 'BLPKF': '-2', 'WHALF': '13', 'G12': '31', 'TOTFRL': '16', 'HISP': '5', 'UGOFFRD': '2', 'HP12M': '0', 'FTE': '1', 'LONCOD': '-80.8626', 'ISPELM': 'PS', 'TR03M': '-2', 'GSHI': '12', 'HP06M': '-2', 'AS01F': '-2', 'TRPKM': '-2', 'AM12M': '0', 'BL07M': '-2', 'HI10F': '0', 'TR02M': '-2', 'AMALM': '0', 'G04OFFRD': '2', 'TR07M': '-2', 'PKOFFRD': '2', 'G07OFFRD': '2', 'AS02F': '-2', 'STITLI': 'N', 'HI01M': '-2', 'BL09M': '1', 'AS01M': '-2', 'HP02M': '-2', 'BL05F': '-2', 'CDCODE': '4502', 'HPUGF': '-2', 'BLKGM': '-2', 'HI05M': '-2', 'HI07F': '-2', 'HI08F': '-2', 'TR09F': '0', 'HPKGM': '-2', 'G06OFFRD': '2', 'BLACK': '39'}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the contents of our list called sample--just the first entry\n",
    "print(sample[0][\"ADDRESS\"], \"\\n\", sample[0][\"URL\"], \"\\n\", sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.richland2.org/charterhigh/', 'https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'https://www.nhaschools.com/schools/rivercity/Pages/default.aspx'] \n",
      " ['RICHLAND TWO CHARTER HIGH', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL', 'RIVER CITY SCHOLARS CHARTER ACADEMY'] \n",
      " ['750 OLD CLEMSON RD, COLUMBIA, SC', '3425 WINTER LK RD LAC1200, WINTER HAVEN, FL', '944 EVERGREEN ST, GRAND RAPIDS, MI']\n"
     ]
    }
   ],
   "source": [
    "# it's VERY important that these three lists be indexed the same, so let's check:\n",
    "print(url_list[:3], \"\\n\", name_list[:3], \"\\n\", terms_list[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Looking at the above, the 3 lists with URLs, names, and search terms, respectively, seem to line up as we would expect: URL[0] corresponds to name[0] corresponds to terms[0], and so on. Now, let's turn these lists into tuples so we can use them for wget. To do this, we use the zip function, which ABSOLUTELY DEPENDS on the consistent indexing we have just established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look great! Now, to use the os library's wget function, we need to manually change directory just as we would in bash/ UNIX. We also need to track which directory we are in and create a folder in which to store the data for each charter.\n",
    "#### Students: be sure to change the directory listed in these commands to one that works for you! Otherwise it will throw an error, and the wget command below won't work. ####\n",
    "\n",
    "To start, we cd into the data directory, and then us getcwd (the equivalent of pwd in bash) to check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.chdir(wget_folder)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Wget commands\n",
    "\n",
    "Note:the trailing slash on the URL is critical – if you omit it, wget will think that papers is a file rather than a directory. x/abc/ means downloading entire x/abc/ page while x/abc means downloading abc file\n",
    "\n",
    "-r recursively download files. it will also follow any other links: if there was a link to somewhere on that page, it would follow that and download it as well. By default, -r sends wget to a depth of five sites after the first one. \n",
    "\n",
    "--no-parent wget should follow links, but not beyond the last parent directory\n",
    "\n",
    "-O  Download and Store With a Different File name\n",
    "\n",
    "-b For a huge download, put the download in background\n",
    "\n",
    "--user-agent mask the user agent by using –user-agent options and show wget like a browser\n",
    "\n",
    "--tries increase retry attempts. Default: 20 times\n",
    "\n",
    "-i Download Multiple Files / URLs. First, need to create a urls.txt file.\n",
    "\n",
    "--mirror download a full website and made available for local viewing\n",
    "\n",
    "-r -A Download only certain file types\n",
    "\n",
    "-e robots=off execute command and ignore robots meta tags and robots.txt\n",
    "\n",
    "--spider determine weather the remote file exist at the destination \n",
    "\n",
    "--domains download only only PDF files from specific domains\n",
    "\n",
    "--user --password download files from password protected sites \n",
    "(wget --user=user --password=password http://www.example.com)\n",
    "\n",
    "--http-user --http-password Download from password protected http sites\n",
    "\n",
    "credits\n",
    "http://programminghistorian.org/lessons/automated-downloading-with-wget\n",
    "https://www.gnu.org/software/wget/manual/wget.html\n",
    "http://www.techsakh.com/2015/11/06/wget-examples-in-linux/\n",
    "http://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(learning_wget)\n",
    "\n",
    "# try different wgets, for learning purposes only\n",
    "#note \n",
    "os.system('wget -m -w 2 --limit-rate=20k http://activehistory.ca')\n",
    "os.system('wget -r -A.pdf http://url-to-webpage-with-pdfs/')\n",
    "os.system('wget -O taglist.zip http://www.vim.org/scripts/download_script.php?src_id=7701')\n",
    "os.system('wget --user-agent=\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3\" http://activehistory.ca')\n",
    "os.system('wget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com')\n",
    "# os.system('wget -O --secure-protocol=auto --no-check-certificate --execute robots=off http://www.berkeley.edu/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now we've got tuples and we're in the right directory to start with. We are finally ready to run wget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing website data for Richland Two Charter High, which is school #1 of 300...\n",
      "Capturing website data for Polk State College Collegiate High School, which is school #2 of 300...\n",
      "Capturing website data for River City Scholars Charter Academy, which is school #3 of 300...\n",
      "Capturing website data for Detroit Enterprise Academy, which is school #4 of 300...\n",
      "Capturing website data for Lighthouse Community Sch Inc, which is school #5 of 300...\n",
      "Capturing website data for Westlake Charter Middle, which is school #6 of 300...\n"
     ]
    }
   ],
   "source": [
    "k=0 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "\n",
    "#testing the first 10 tuples\n",
    "tuple_test = tuple_list[80:]\n",
    "\n",
    "for tup in tuple_list:\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", (tup[1].title()) + \", which is school #\" + str(k), \"of 300...\")\n",
    "    # use the tuple to create a name for the folder\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + (tup[1].title())\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + (tup[1].title())\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + (tup[1].title())\n",
    "    os.chdir(wget_folder)  # cd into data directory--this is a/\n",
    "    if not os.path.exists(dirname):\n",
    "    # key step because otherwise wget puts the output into whatever folder it was previously in, can get real messy.\n",
    "        os.makedirs(dirname) # create the folder using the dirname we just made, then change into that directory\n",
    "    os.chdir(dirname)  # other options to think about for wget: --page-requisites --retry-connrefused --convert-links --wait=3\n",
    "    \n",
    "    #trying different wgets\n",
    "    if(False):\n",
    "        os.system('wget -i -r -m --level=3 --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png, .PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "         .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX ' +(tup[0]))\n",
    "    if(False):\n",
    "        os.system('wget -pn --mirror --random-wait  -A '+  (tup[0]))\n",
    "    if(True):\n",
    "        os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "          --accept .html' + ' ' + (tup[0]))\n",
    "        with open(\"Output.txt\", \"w\") as text_file:\n",
    "        text_file.write(\"Purchase Amount: %s\" % TotalAmount)\n",
    "    if(False):\n",
    "        os.system('wget --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "        --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "        --reject .mov,.MOV,.avi,.AVI,.mpg,.MPG,.mpeg,.MPEG,.mp3,.MP3,.mp4,.MP4,.png,.PNG,.gif,.GIF,.jpg,.JPG,\\\n",
    "        .jpeg,.JPEG,.pdf,.PDF,.pdf_,.PDF_,.doc,.DOC,.docx,.DOCX,.xls,.XLS,.xlsx,.XLSX,.csv,.CSV,.ppt,.PPT,.pptx,.PPTX' + ' ' + (tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting wget: what if there's no directory hierarchy?\n",
    "Although it's not common practice, some sites have no directory hierarchy, and this makes it difficult to scrape them automatically. For instance, Chinook Montessori Charter School's home URL is 'http://www.k12northstar.org/chinook', and if you go to this site you'll find links to other website pages with URLs that are NOT nested within this home URL, e.g. 'http://www.k12northstar.org/Page/2684' and 'http://www.k12northstar.org/Page/2685'. \n",
    "\n",
    "Our problem with directory-less URL structures is that the coding strategy used above really depends on wget's \"recursive\" option, which scrapes the site you give it as well as all sites included within that. For instance, it would scrape not only www.school.com/ but also www.school.com/cafeteria and www.school.com/about_us. This won't work for sites with no directory hierarchy. It may be impossible to separate out the site pages for only the school of interest if the site directory includes multiple schools at a \"flat\" level with no hierarchy... But we won't know until we try!\n",
    "\n",
    "Let's look at coding an automated solution to deal with this problem. Thus far, I've only thought up a painstaking solution that involves feeding wget each site manually. Below is an example of that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grabbing #142--a school that has no directory hierarchy in its website:\u001f\u001f\n",
    "os.chdir(no_dir_folder)\n",
    "\n",
    "for url in ['http://www.k12northstar.org/chinook', 'http://www.k12northstar.org/site/Default.aspx?PageID=2678', \\\n",
    "'http://www.k12northstar.org/Page/2684', 'http://www.k12northstar.org/Page/2685', \\\n",
    "'http://www.k12northstar.org/Page/2686', 'http://www.k12northstar.org/Page/2683', \\\n",
    "'http://www.k12northstar.org/Page/2704']:\n",
    "    os.system('wget --no-parent --show-progress --progress=dot --convert-links \\\n",
    "        --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "        --user-agent=\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:11.0) Gecko/20100101 Firefox/11.0\" \\\n",
    "         --accept .html' + ' ' + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function for getting parents' links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:\n",
    "- run wget on URL L\n",
    "- if wget on L captured no HTML:\n",
    "    - if L has a parent that's a valid URL (call it P):\n",
    "        - wget on P, set k=1\n",
    "    - if wget on P captured no HTML:\n",
    "        - if P has a parent that's a valid URL (call it D):\n",
    "        - wget on D, set k=2\n",
    "    - if wget on D captured HTML, repeat again (total 5 nested ifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not needed for wget\n",
    "# just tested things out, this method might be useful?\n",
    "def get_parent_link(str):\n",
    "    \"\"\"Function to get parents' links. Return a list of valid links.\"\"\"\n",
    "    return get_parent_link_helper(3, str, []);\n",
    "\n",
    "def get_parent_link_helper(level, str, result):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to get parent link of a given link. Return a list of urls \"\"\"\n",
    "    if level == 0 or not check(str):\n",
    "        return result\n",
    "    else:\n",
    "        result += [str]\n",
    "        return get_parent_link_helper(num -1, str[: str.rindex('/')], result)\n",
    "\n",
    "#needed for wget\n",
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        return True\n",
    "    except urllib.error.URLError:\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.k12northstar.org/chinook', 'http://www.k12northstar.org']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parent_link(\"http://www.k12northstar.org/chinook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified version for wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wget(str):\n",
    "    \"\"\"wget a link or its parent link. Default: 5 levels\"\"\"\n",
    "    return wget_helper(5, str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/Users/anhnguyen/Desktop/research/scraping_Python/parent_lins_tests\"\n",
    "def wget_helper(level, str):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to wget on url and its parents \"\"\"\n",
    "    \n",
    "    if level == 0 or wget_success(level, str):\n",
    "        return\n",
    "    else:\n",
    "        return wget_helper(level -1, str[: str.rindex('/')])\n",
    "    \n",
    "def wget_success(level, link):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "    #check if the link exists\n",
    "    if not check(link):\n",
    "        return False\n",
    "    #create folder, ready for wget\n",
    "    os.chdir(path)\n",
    "    folder_name = str(level) +\"_\"+ link[12:]\n",
    "    os.makedirs(folder_name)\n",
    "    os.chdir(folder_name)\n",
    "    os.system('wget {}'.format(link))\n",
    "    \n",
    "    #did wget get anything?\n",
    "    list_of_files = os.listdir(path + '/'+folder_name)\n",
    "    if list_of_files == []:\n",
    "        return False\n",
    "    \n",
    "    #is there a html file\n",
    "    if (True in [file_name.endswith('.html') for file_name in list_of_files]):\n",
    "        return True\n",
    "    return False     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wget('http://www.k12northstar.org/chinook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try to write to text file\n",
    "success_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/success.txt\"\n",
    "fail_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/fail.txt\"\n",
    "with open(success_file, \"a\") as text_file:\n",
    "    text_file.write(\"test 3\" +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
